---------------------------------------------------------------------------------------------------------------
kubernetes version: 1.23
---------------------------------------------------------------------------------------------------------------
https://docs.aws.amazon.com/secretsmanager/latest/userguide/integrating_csi_driver.html
https://waswani.medium.com/integrating-secrets-manager-with-aws-eks-79c93e70c74e

Kubernetes:
It is an open-source container orchestration platform that automates many of the manual processes
involved in deploying, managing, and scaling containerized applications.

There are 2 types of creating Kubernetes clusters
1. Bare metal cluster.
2. Managed Kubernetes cluster.
EKS (Managed by AWS), AKS (Managed by Azure)
Kubernetes was first developed by google in Golang language now it is managed by CNCF (Cloud-native Computing Foundation)
Cgroups limit the memory and CPU utilization of a container.
Namespaces where restrict the container so that it can only see how much has been created by a container.
Kubectl --> (Kubernetes control line commands) To run Kubernetes commands on the cli.
---------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------
10-12-22 - class 2
---------------------------------------------------------------------------------------------------------------
This should be run in the sudo user of the worker nodes

( kubeadm join 172.31.39.185:6443 --token 06hl52.bodwn0mucdiwbazi \
        --discovery-token-ca-cert-hash sha256:2939520c434d1c48dbd189bcd87fc9b5dcd58ebd39f4cfe9ac5253708171b9e0 ) --> Generated by me after kubedam init

Installation:

Link: https://github.com/artisantek/kubernetes-installation

kubeadm init will initilise all the componenets (API server, Scheduler, controller-manager, etcd).

Once the initilisation is done we will be getting the kubeadm joint command where we should perform this action on 
the worker machines to join & comunicate with the master where we will be using for autencialtion purpose 
and this key will be valid only for 1month after we need to extratc it again.

kube proxy will assigne ip to the containers inside the pods, & Weaveworks will make shure to have an unique ip addresses
for the containers that are in the pods.

kubectl get nodes --> To check all the nodes on the cluster
kubeadm token create --print-join-command --> To create kubeadm join command
kubectl label node <node-name> node-role.kubernetes.io/worker1= --> To give a role to a node

kubectl label node <node-name> node-role.kubernetes.io/<any_name>= --> To give a role to a node
Here
 '=' Key & pair name both are same
---------------------------------------------------------------------------------------------------------------
Kubernetes Architecutre

The master mainly consists of 4 parts
1. API Server.
2.Scheduler.
3.Controller-Manager
4.etcd
This master can connect to as many number of workder nodes
These worker nodes consists of 4 parts
1. Pods
2. Container runtime (Docker)
3. Kublet
4. kube-proxy

1. API Server.
Main managing point of the kubernets cluster.
In Kubernetes, every API call needs to authenticate with the API server, regardless of whether it comes from outside the
cluster, such as those made by kubectl, or a process inside the cluster, such as those made by kubelet. And it will
also checks whether the user is permitted to perform the requested action.

2.Scheduler.
In simple words scheduler will decide where the pods should run inside the kubernetes cluster based on the 
resource avalibility on the worker nodes.
It will select on which pod that the containers should be created based on the resource availability.

3.Controller-Manager.

Which runs a never-ending core control loop basically, a controller watches the state of the cluster 
through the API Server watch feature and, when it gets notified, it makes the necessary changes attempting 
to move the currentstate towards the desired state.
in silmple words Which will run a never ending controle loop that will track all the changes that are happening in the cluster 
and make sure to maintain all the requirements of the user.
The relation between controller-manager and api server is api watch mechanism.

Different types of deployment controllers.
1. Deployment.
2. Replication Controllers.
3. Demon set.
4. Statefull set.

kubctl get deploy --> To check the deployer controller we have.
kubctl delete deploy <deployment-controller-name> --> To delete the deployment controller we have in cluster.

4. etcd
etcd reliably stores the configuration data of the Kubernetes cluster (Ip addresses, labels, configurations).

Parts of worker nodes.
1. Pods 
Thses are the smallest deployable units in the kubernetes which will host one or more containers.

2. Container Runtime
Where it will helps kubernetes to create containers inside the pod.

3. kuelet
kubectl is the primary means by which a developer can interact with a Kubernetes cluster.
Kubelet on the other hand is a process that runs on each node of a Kubernetes cluster and creates, destroys, 
or update pods and their Docker containers for the given node when instructed to do so.

4. Kube proxy
This is a proxy service which runs on each node and helps in making services available to the
external host and inside the kubernetes cluster and allocates unique ip addresses for the containers
inside the pods.
---------------------------------------------------------------------------------------------------------------
How Ip's are assigned to the pods:

---------------------------------------------------------------------------------------------------------------
Pod Definition File:

apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 80
---------------------------------------------------------------------------------------------------------------
Commands:
kubectl --> Kuberenets controle line command
kubectl apply -f <file>.yaml --> To apply a configuration in kubernetes
kubectl get pods --> To list the pods in the cluster
kubectl get pods <boject-name>-o yaml --> To see the configuration file as output.
kubectl get pods -0 wide --> Get details of the pods such as pod and container ip addresses
kubectl get deploy --> To check the deployer controller we have.

kubectl describe <object-kind> <name> --> To see information about a kubernetes object
kubectl delete <object-kind> <name> --> To delete a kubernetes object
---------------------------------------------------------------------------------------------------------------
traceout <pod_ip> to connect from one pod to other pod
---------------------------------------------------------------------------------------------------------------
11-12-22 - class 3
---------------------------------------------------------------------------------------------------------------
Manifest File Fields/kubernetes configuration files:

1. apiVersion:
It specifies the version of kubernetes api to be used for creating kubernetes objects.It can be v1, apps/v1 etc.

ex: v1, apps/v1 etc

kubectl api-versions --> To list the api versions in kubernetes
kubectl api-resources | grep <api-version-name> --> To check the objects that can be created by a particular api version
kubectl api-resources --api-group <group_name> --> To check the objects that can be created by a perticular version.

2. kind:
It specifies the type of kubernetes object to be created it can be pod, service etc.

ex: Pod, Service, Deployment etc

3. metadata:
It is used to set the information about the ojects like name, namespace under which object will be running.

4. spec: 
It consists of core information i,e the desired state of the object.
ex: containers ,images etc

5. labels:
There are key value pairs attached to an kubernetes object which are usefull in grouping & selecting the objects. 

ex:
labels:
  app: nginx
  os: ubuntu
kubctl get pods -l app=nginx --> To select and identify with a perticular label.

6. Selectors (Mainly used in Replicasets and deployment controllers)

They are used to identify the kuberetes objects using there labels.
There are 2 types of selectors in kubernetes
a. Equity based selectors.
Used to identify objets by key & an exact value.
Operataors Allowed are: =, ==, !=

Example: 
app = nginx
os = ubuntu
kubctl get pods -l app=nginx

b. Set-Based Selectors
Used to identify objects by keys based on a set of values.
Operataors Allowed are: in, notin, exists

Example:
app in (nginx, tomcat, jenkins)
environment in (uat, production)

kubctl get pods -l 'app in (nginx, web, tomcat)'
---------------------------------------------------------------------------------------------------------------
ReplicaSet/ReplicationController:

These are used to create the multiple pods with the same speciations at a time

Difference between ReplicaSet and ReplicationController:

[ a.ReplicaSet
supports both set-based & equity based selectors, where it supports all the lasted api-versions.
When we menction replicaset value as 3 it will make sure that these many number of pods will
be running on the worker nodes.

Selector is very important to mention inside the manifest file because how can this replicaset will understand that
this pod is created under a perticular repliaset and to manage them (When ever we mention the 3 no of replicas the
replicaset will make sure that those many pods with the necessary sepcs should be running inside the working nodes and
any pod goes down it will identify based on the selector which we are mentioned inside the manifest file & it will create 
another pod with same specs)

b.ReplicationController.
supports Equity based selectors its an older version where it suppots only v1 api-version ]
  
ReplicaSet/ReplicationController
There are used to create multiple instances of a single pod to acive load balancing and high availability.If any pod gets deleted
then the controller will recreate the new one.

Difference between ReplicaSet and ReplicationController:(Re-create update strategy is done by replicaset) 
Both are used to create multiple replicas of pods but the replicationcontroller only supports equity selectors while the replicaset 
supports both equity based and set based selectors.
---------------------------------------------------------------------------------------------------------------
Replicaset:
A ReplicaSet is a type of controller in Kubernetes, used for ensuring that a specified number of replicas of pods
are running at any given time. If a pod crashes or becomes unavailable, the ReplicaSet automatically creates a 
new replica to take its place. It will also helps in load balancing.

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: nginx-rs
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      name: nginx
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80

kubectl scale <Api-object_name> <object_name> --replicas=5 --> To scale the pods
ex: kubectl scale rs nginx-deploy --replicas=5 --> To scale the pods
---------------------------------------------------------------------------------------------------------------
Deployment Controller:(Rolling update strategy)
It is the most common object used to deploying objects in kubernetes.With deployment controller we can efortlessly
role-out application updates and roll-back the updates without breaking the user experience.

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      name: nginx
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80

Commands:

kubectl set image <api-object-name> <object-name> <container-name>=<New-image-name> --record --> To update the image using Deployment Controller
ex: kubectl set image deploy nginx-deploy nginx=nginx:1.14 --record 

kubectl rollout history <api-object-name> <object-name>
ex: kubectl rollout history deploy nginx-deploy --> To check the total recorded hostory of the Controller

kubectl rollout undo <api-object-name> <object-name>
ex: kubectl rollout undo deploy nginx-deploy --> To roll back to a previous version of the application

kubectl rollout undo <api-object-name> <object-name> --to-revision=<Version> --> To roll back to a particular revision number
ex: kubectl rollout undo deploy nginx-deploy --to-revision=<Version_number>

kubectl scale <Api-object_name> <object_name> --replicas=5 --> To scale the 
ex: kubectl scale deploy nginx-deploy --replicas=5 --> To scale the pods

---------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------
12-12-22 - class 4
---------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------
DaemonSet Controller.

Daemonset controller ensures that a pod runs on all the nodes of the cluster if a node is added or reomved from
cluster daemonset automatically adds or deletes the pods respectivelly.

Typicall use cases for daemonset are
1. Monitoring apps likes node exporter, prometheus use daemonset to moniter all the nodes in the cluster.
2. logs collection: Fluentd application collects the logs from all the nodes by using daemonset.

apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: nginx-daemon
spec:
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      name: nginx
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
---------------------------------------------------------------------------------------------------------------

Statefullset controller

Like a deployment controller statefullset also manages pods that are based on an identical container specification
unlike deployment statefullset maintance unique identity with each of there pods i,e the pods that are created will
have there own state and there own volume.
Which as has it's own state and volume but laks in proxy when a pod is deleted it will get created on the same workernode
with the same name. which will have unique DNS name.
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: nginx-hl
spec:
  serviceName: head
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      name: nginx
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: hl
spec:
  clusterIP: None
  ports:
  - targetPort: 80
    port: 80
  selector:
     app: nginx

apt-get update
apt-get install dnsutils
nslookup --> Enter the command
Enter the DNS name of the statefullSet controller
---------------------------------------------------------------------------------------------------------------

Pod phases/pod lifecycle.

The phase of a pod is a simple high level summery where the pod is in its life cycle.

1. Pending.
The pod has been accepted by the kubernetes cluster (Api srever) but one or more containers present inside the pod have not yet been started.

2. Running.
The pod has been scheduled to a node and all the containers have been created and they are upend running.

3. Successed.
All the containers in the pod have terminated in success and will not be restarted. (Task inside the container is completed
and it will no more get started)

4. Failed.
All the containers in the pod have terminated but atleast one container has termined in failure i,e that the
container exied non-zero value.

5. Unknown.
The pod goes to unkown state if the state of the pod couldnot be obtained due to an error in communicating with the node 
where the pod was running.

---------------------------------------------------------------------------------------------------------------
13-12-22 - class 4
---------------------------------------------------------------------------------------------------------------
Probes:

Probes are the actions performed by the kubelet which help in monitering and checking the condition of the
applications inside the pods.
There are defined at the container level and we can define multiple probes in the manifest file.

Probe Types:

1. Readiness Probe:

With readiness probe if the conditions configured in the manifest failed the appcation/pod are not allowed to serve
external request.
(Once the pod inside the nod gets non-functional then readiness probe will removes the container inside the pod)

2. Liveness Probe:
Rater the stoping the pods to serve the external request like readiness prob liveness prob in case of failure 
it kills the container and re-starts it which might helps the application to get back to it's initial healthy state.

Summary
Both liveness & readiness probes are used to control the health of an
application. Failing liveness probe will restart the container, whereas failing
readiness probe will stop our application from serving traffic.
---------------------------------------------------------------------------------------------------------------
Configuring Probes:

1. initialDelaySeconds [Default-0, Min-0]:
The number of seconds between the container starts and firt probe action.
(How much time should probe wait initially befor sending the probe action)

2. timeoutSeconds [Default-1, Min-1]:
It is the number of seconds the probe waits for a responce from pod before assuming failure.
(How much time should probe should wait to get reponse from the pod)

3. periodSeconds [Default-10, Min-1]:
It is the frequency (in seconds) of probe actions of initial delay time
(How much time the probe should wait before sending the another request)

4. successThreshold [Default-1, Min-1]:
The number of conssegitive positive responce needed to switch probe status to success.
(Number of consigutive sessive probe actions to get before concluding that the application inside the pod is working)

5. failureThreshold [Default-1, Min-1]:
The number of conssegitive negative responce needed to assume the pod has failed.
(Number of consigutive failure probe actions to get before concluding that the application inside the pod is failed)
---------------------------------------------------------------------------------------------------------------
Probe Actions:
This probe action is used to run shell commands inside the pods and the responce is considered failure if the command
exits with non-zero value.

1. Shell[exec]:

Syntax:

<probe-type>
  exec:
  command:
  - command1
  - command2

Example:

apiVersion: v1
kind: Pod
metadata:
  name: alpine
  labels: 
    app: alpine
spec:
  containers:
  - name: alpine
    image: alpine
    args:
    - /bin/sh
    - -c
    - touch /home/probecheck; sleep 15; rm /home/probecheck; sleep 10000000
    livenessProbe:
      exec:
        command:
        - cat
        - /home/probecheck
      initialDelaySeconds: 2
      periodSeconds: 3
      failureThreshold: 3
---------------------------------------------------------------------------------------------------------------
2. HTTP Request [httpget]:
This probe action sends an http get request to the path defined inside the probe the http responce code determines
whether the probe id successfull or not.

Syntax:

<probe-type>
  httpGet:
    path: <path-inside-container>
    port: <port>

apiVersion: v1
kind: Pod
metadata:
  name: liveness
  labels: 
    app: probe
spec:
  containers:
  - name: liveness
    image: registry.k8s.io/liveness
    args:
    - /server
    livenessProbe:
      httpGet:
        path: /healthz
        port: 8080
      initialDelaySeconds: 2
      periodSeconds: 3
      failureThreshold: 3
---------------------------------------------------------------------------------------------------------------
HTTP (Hypertext transfer protocol)  Status Codes (https://restapitutorial.com/httpstatuscodes.html)
mostly founding errors(https://www.webfx.com/web-development/glossary/http-status-codes/what-is-a-503-status-code/)

1.Informational Responce. (100-199)

2.Successful responce.(200-299)

a) 200-Ok
The server has replied to the client and given the client the documents.

b) 201-Created
The 201 Created status code indicates that the request was successfully fulfilled, resulting in the creation of
one or more new resources.

c) 204-No content
HTTP request has been successfully completed, and there is no message body.

3. Redirection Responce.(300-399)

a) 301- Permement Redirect
Status code 301 is shown when a page has been moved, permanently replaced a URL with another URL.

b) 302- Temporary Redirect
The resource requested has been temporarily moved to the URL given by the Location header

c) 304-Not Modfied.
An HTTP 304 not modified status code means that the website you're requesting hasn't been updated
since the last time you accessed it. Typically, your browser will save (or cache) web pages so it
doesn't have to repeatedly download the same information.

4. Client error Responce(400-499)

a) 400-Bad Request
A good case in point is a URL string syntax error. Incorrectly typed URLs,
or URLs that contain backslashes and other invalid characters can garble a request.

b) 401-unauthorized
It may occur client does not provide the proper authentication credentials to the server within the request time

c) 403-Forbidden
A 403 Forbidden Error occurs when you do not have permission to access a web page or something else on a web serve

d) 404- Not Found
--> when website content has been removed or moved to another URL
--> The URL was written incorrectly (during the creation process or a redesign), linked incorrectly,
or typed into the browser incorrectly

e) 405- Method not allowed
This code response confirms that the requested resource is valid and exists, but the client has used an 
unacceptable HTTP method during the requests

f) 414- Url too long
The client understands that the server expects a smaller HTTP path or address

g) 415- unsupported media type.
The HTTP Status Code 415 means that the request entity has a media type that the server or resource does not support.
eg: One example of a situation that would cause a 415 status code is if the client uploads an image into one format
(e.g., PNG) but the server requires another format (e.g., JPG).

5. Server error Responce.(500-599)

a) 500-internal server error
The server encountered an unexpected condition that prevented it from fulfilling the request.

b) 501-Not implemented 
The server does not support the functionality required to fulfill the request.

c) 502- Bad gateway
The server, while acting as a gateway or proxy, received an invalid response from an inbound server 
it accessed while attempting to fulfill the request.

d) 503- Server unavalible
The server is currently unable to handle the request due to a temporary overload or scheduled maintenance,
which will likely be alleviated(ನಿವಾರಿಸಲಾಗಿದೆ) after some delay.

e) 504- gateway timeout.
while acting as a gateway or proxy, did not get a response in time from the upstream server that it 
needed in order to complete the request.

---------------------------------------------------------------------------------------------------------------
3. TCP Port Check [tcpSocket]:
This probe action is used to check if a port of the pod is open and if the kubelet can connect to that specfic port
Syntax:

<probe-type>
  tcpSocket:
    port: <port-number>

apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels: 
    app: nginx
spec:
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 80
    readinessProbe:
      tcpSocket:
        port: 100
---------------------------------------------------------------------------------------------------------------
4. Startup Probe [1.16]:
Both startup and liveness probe have same properties i,e in case of failure they restarts the container.
If bothstartup and liveness probes are set after container creation kubelet will execute startup probe first
if startup probe successed the liveness probe takes over further actions.
If startup probe fails the pod is restarted then the cycle is repeated it is an extra layer of check for the pods.
---------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------
14-12-22 - class 5
---------------------------------------------------------------------------------------------------------------
Service:
In kubernetes pods can communicate with each other by default without any confirmation by using internal ip_addresses
assigned by kube proxy, this ip addresses are volatile(Keeps on changing) in nature.
A kubernetes service groups a series of pods together and makes them accessible through common host name or ip_address.
---------------------------------------------------------------------------------------------------------------
Need for kubernetes service.
--> Since pods are short lived i,e a pod dies and a new pod comes up the ip_address for the pod is most likely to change
therefore we cannot relay on pod ip_address.
--> We cannot use the ip_address of the pod if we want to conncet to the application from the outside world.
--> pods can be scaled up and there is no way to know the new ip_address of the pod in advance.
--------------------------------------------------------------------------------------------------------------
Adventages of services in kubernetes

--> Stable ip address to the pods (when ever the pod restarted it will be assigned with a new ip address by using the services 
we can get the stable ip address to the pod and container)
--> Service can also helps in load balancing when ever the user tries to connect to an application the load will be
distributed among the pods present inside the kubernetes cluster)
--> They helps in establishing the communication among the pods inside the cluster
--> Establish the communication for out side.
---------------------------------------------------------------------------------------------------------------

Target Ports: Ip addresses on which port applications inside the pod is running.
Port: The Ip address on which the service (eg cluster ip) container is running.
Node port: The port on which the appications are exposed to the external environment.
Generally we give application ip and the port ip both same to avoid confusions.
---------------------------------------------------------------------------------------------------------------

Types of Service:
1. Cluster IP 

Cluster ip is the default service type in kubernetes & it exposes the pods internally with in the cluster,
it enables pods to communicate to other pods inside the cluster through a common ip and DNS (cluster IP name)

apiVersion: v1
kind: Service
metadata:
  name: cip
spec:
  type: ClusterIP
  ports:
  - targetPort: 8080
    port: 8080
  selector:
    app: helloworld

curl <pod iP-address>:<port on which application is running> --> To check whether we are getting the responce form all
the pods that containing application without getting inside the container.

curl <service_name>:<port on which application is running> -->To check whether we are getting the responce form all
the pods that containing application by getting inside the container.
---------------------------------------------------------------------------------------------------------------
2. NodePort:

With noteport service we can expose pods in the cluster to the outside world

Range: 30000-32767

apiVersion: v1
kind: Service
metadata:
  name: np
spec:
  type: NodePort
  ports:
  - targetPort: 8080
    port: 8080
    nodePort: 31000
  selector:
    app: helloworld
---------------------------------------------------------------------------------------------------------------
3. LoadBalancer
Loadbalancer service will create an internal kubernetes service that is connected to loadbalancer provided by any
cloud provider. With loadbalancer service nodeport and clusterIP services are created automatically.

(If you don't want to use the ip addresses to connect with the application externally trough (www) because the
ip_addresses will be getting changed then we can use loadbalancer.
Where it will have a stagnant DNS thorugh which we can establish the connection to cluster ip & this will establish
connection between the pods.
Rather then using the cloud load balancer we can use internal load balancer called ingress load balancer in kubernetes.)
---------------------------------------------------------------------------------------------------------------
4. Headless Service:
Headless service is same as default clusterIP but lacks proxing allowing us to connect to a pod directly.
headless service will written all the ip's of the pods which are associated with it.
(For this to work we should create pods with statefullSet)
Headless service is used to connect to a perticular pod rather then selecting multiple pods.

apiVersion: v1
kind: Service
metadata:
  name: hl
spec:
  clusterIP: None
  ports:
  - targetPort: 80
    port: 80
  selector:
    app: nginx
---------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------
Pods horizontal autoscaling - https://youtu.be/3BnrXapY7zo
Documentation - https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/
cluster horizontal autoscaling - https://youtu.be/lMb6wzy0PPA
Documentation - https://docs.aws.amazon.com/eks/latest/userguide/autoscaling.html

apiVersion: apps/v1
kind: Deployment
metadata:
  name: php-apache
spec:
  selector:
    matchLabels:
      run: php-apache
  template:
    metadata:
      labels:
        run: php-apache
    spec:
      containers:
      - name: php-apache
        image: registry.k8s.io/hpa-example
        ports:
        - containerPort: 80
        resources:
          limits:
            cpu: 500m
          requests:
            cpu: 200m
---
apiVersion: v1
kind: Service
metadata:
  name: php-apache
  labels:
    run: php-apache
spec:
  ports:
  - port: 80
  selector:
    run: php-apache

command :kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10
---------------------------------------------------------------------------------------------------------------
out of syllabus

1) port forwading:
kubectl port-forward serivces/nginx-services <port on which it is working>:<port to which to be forwarded>

2) To get the nod ip
minikube ip -p local-cluster 
---------------------------------------------------------------------------------------------------------------
Image: artisantek/k8s-helloworld
---------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------
15-12-22 - class 6
---------------------------------------------------------------------------------------------------------------
Manually Scheduling Pods:

By default pods scheduling in kubernetes handled by the scheduler, the shceduler ensures the right is seclected
by checking the resources avalible on the worker nodes, how ever their are scenario where you want your pods to 
end up on certain specific nodes.
--> To schedule pods on nodes with specific resources available on them.(eg: Higher ram, storage, SSD)
--> To co-locate a pod from one service with a pod from another service on the same node due to stronge dependency
with each other.

There are 3 types of manually scheduling the pods.
1. Node selector.--> (schedule for a particular node)
2. Affinity--> (schedule for a particular node)
a. Node affinity
b. Pod affinity & anti affinity
(pod affinity is used to co-locate a pod which are dependend i,e a web application will be attached to an cache
data to store data and for easy access)
(Anti affinity is used where if you doesn't want to co-locate the pods)
3. Taints and Toleration. --> (Repeal from a particular node)

1. nodeSelector:
It is the simplest way of scheduling the pods on to a particular node by using the node labels. 

apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels: 
    app: nginx
spec:
  nodeSelector:
    spec: high
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 80
---------------------------------------------------------------------------------------------------------------
Commands:

kubectl get nodes --show-labels --> To display all the labels associated with the nodes
kubectl label node <node-name> <key>=<value> --> To add a label to a node
kubectl label node <node-name> <key>- --> To remove a label
eg: kubectl label node <ip_address of node> <label_name>-
---------------------------------------------------------------------------------------------------------------
2. affinity:
The affinity greatly expands the node selector by adding soft and hard scheduling rule, if the soft rule is not 
met the scheduler can still schedule a pod onto some node.

-->With affinity we can mention two or more labels so that pod will be scheduled on either of one by the resourece
availability on the nodes.
--> With affinity we can even execlude the scheduling for a set of nodes inside the cluster.

Hard Rule[requiredDuringScheduling]:
Rules must be met compulsory for a pod to be scheduled onto the node.

Soft RUle[preferredDuringScheduling]:
Scheduler will try to schedule the pods based on the preference defined, if schedulers doesnot find any matching node
it will schedule the pods on any nodes.

IgnoredDuringExecution:
If labels on a node change during run time such that the node ceases to satisfy the affinity rules the pods will
still continue to run on the node.

---------------------------------------------------------------------------------------------------------------
Types:

a. nodeAffinity:
Like node selectors we can use nodeaffinity to schedule pods on to a specfic nodes.

1. Hard Rule

eg:1

apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels: 
    app: nginx
spec:
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 80
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: spec
            operator: In
            values: 
            - high
            - medium

eg:2
												
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels: 
    app: nginx
spec:
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 80
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: spec
            operator: NotIn
            values: 
            - low
eg:3

apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels: 
    app: nginx
spec:
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 80
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: spec
            operator: NotIn
            values: 
            - high
          - key: type
            operator: In
            values: 
            - ssd

With eg2 the pod will be either created in label with high or medium but it won't be created in the node with
label low

(requiredDuringSchedulingIgnoredDuringExecution:) --> i,e conditions should met for scheduling the pod if the 
label conditions are not met then schedulers will unable to creat the pod.

meaning:
Hardrule: --> The labeling conditions should be meet to create a pod on a
perticular node once the label conditions are not met scheduler will not be able to create the pod on the worker node.

(Important
If we want to create a pod with label low and type as SSD we need to create another key under matchexpression.)
---------------------------------------------------------------------------------------------------------------
2. Soft Rule

apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 80
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: spec
            operator: In
            values:
            - type
      - weight: 3
        preference:
          matchExpressions:
          - key: spec
            operator: In
            values:
            - high

meaning:
Softrule: --> The labeling conditions should be meet to create a pod on a perticular node if the conditions
that are mentioned are not met then the scheduler will create pod on any one of the working node inside the cluster
based on the resource availability inside the nodes.
---------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------
16-12-22 - class 7
---------------------------------------------------------------------------------------------------------------
Pod Affinity and Anti Affinity: (good explaination: https://youtu.be/I7t57_tQBz4)

With podaffinity and antiaffinity we can define wether a pod should or shouldnot be scheduled on to a perticular
node based on labels of other pods already running on that node.

Deploy DB with nodeSelector:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: postgres
spec:
  replicas: 1
  selector:
    matchLabels:
      app: db
  template:
    metadata:
      name: db
      labels:
        app: db
    spec:
      nodeSelector:
        spec: high
      containers:
      - name: db
        image: nginx
        ports:
        - containerPort: 80
---------------------------------------------------------------------------------------------------------------
Deploy Web app with Anti-Affinity

apiVersion: apps/v1
kind: Deployment
metadata:
  name: web
spec:
  replicas: 2
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      name: web
      labels:
        app: web
    spec:
      containers:
      - name: web
        image: nginx
        ports:
        - containerPort: 80
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - web
            topologyKey: kubernetes.io/hostname
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - db
            topologyKey: kubernetes.io/hostname
---------------------------------------------------------------------------------------------------------------
Deploy Redis App with Affinity:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      name: redis
      labels:
        app: redis
    spec:
      containers:
      - name: redis
        image: nginx
        ports:
        - containerPort: 80
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - redis
            topologyKey: kubernetes.io/hostname
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - web           
            topologyKey: kubernetes.io/hostname
---------------------------------------------------------------------------------------------------------------
Taints and Tolerations:
Affinity is a property of pods that attracts them to a set of nodes while the taints are the opposite they allow
a node to repeal a set of pods.
If a pod must be deployed on a tainted node tolerations are added to the pod defination file.


Commands:
kubectl get nodes -o jason | grep "kubernetes.io/hostname" -c 20 --> To check all the nodes that are tainted in cluster
kubectl taint node <node-name> <key>=<value>:<taint-action> --> To taint a node
eg:
kubectl taint node 172-31-43-109 taint=w1:NoSchedule
kubectl taint node <node-name> <key>=<value>:<taint-action>- To untaint a node
---------------------------------------------------------------------------------------------------------------
Taint Actions:

1. NoSchedule:
Unless a pod has the matching toleration it won't be scheduled on to the node.

2. PreferNoSchedule:
It is the soft version of the noschedule.

3. NoExecute:
Kubernetes will immediately evict all the pods without the matching tolerations from the node.

---------------------------------------------------------------------------------------------------------------
Pod With Tolerations:

apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 80
  tolerations:
  - key: taint
    operator: Equal
    value: w1
    effect: NoSchedule
---------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------
16-12-22 - class 8
---------------------------------------------------------------------------------------------------------------
Kuberenetes Volumes:

It is a directory that is attached to a pod and mounted to one or more containers inside a pod.

Simple Mount:

A hostPath volume mounts a file or directory from the host node’s filesystem into your pod.
A hostPath PersistentVolume must be used only in a single-node cluster. Kubernetes does not support hostPath
on a multi-node cluster currently.
We cannot limit the data storage in this method.

apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  volumes:
  - name: simple-mount
    hostPath:
      path: /tmp/simplemount/
      type: DirectoryOrCreate
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 80
    volumeMounts:
    - mountPath: /home
      name: simple-mount
---------------------------------------------------------------------------------------------------------------
Ephemeral Volumes (EmptyDir):

They are short lived they are tightly depenedent with the life time of the pods and they are deleted if the
pod goes down.
---------------------------------------------------------------------------------------------------------------
Persistent Volumes:

Persistent volumes are ment for long term storage and are independent of the pods life cycle.

(A PersistentVolume (PV) in Kubernetes is a pool of pre-provisioned storage resources in a Kubernetes cluster,
that can be used across different user environments)


Types of Persistent Volumes:

1. Static PV (Persistent Volumes):

In static provisioning an adminstrator manually creates pools of persistance volumes.

PV (Persistent Volumes)--> PVC (Persistent Volumes claims) --> Pod ==> workflow

Persistent Volume:

apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv
spec:
  reclaimPolicy: Recycle
  capacity:
    storage: 2Gi
  hostPath:
    path: /tmp/simplemount/
  storageClassName: static
  accessModes:
  - ReadWriteOnce
---------------------------------------------------------------------------------------------------------------
Persistent Volume Claim:

With Persistent Volume Claim we can use persistent volume pools created by an administrator.
Persistent volume claim are a way for a developer to request storage for the application.

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc
spec:
  storageClassName: static
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi
---------------------------------------------------------------------------------------------------------------
Mounting PVC to Pod

apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  volumes:
  - name: persistent-volumes
    persistentVolumeClaim:
      claimName: pvc
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 80
    volumeMounts:
    - mountPath: /home
      name: persistent-volumes
---------------------------------------------------------------------------------------------------------------
2. Dynamic Volumes:

In dynamic volumes we configure storage classes to dynamically create persistent volumes. The main goal of storage
class is to elaminate the need for administrator to pre-provision storage.

---------------------------------------------------------------------------------------------------------------
Storage Classes types:
type: io1, gp2, sc1, st1. See AWS docs for details. Default: gp2.
---------------------------------------------------------------------------------------------------------------

The access modes:

ReadWriteOnce -- the volume can be mounted as read-write by a single node
ReadOnlyMany -- the volume can be mounted as read-only by many nodes 
ReadWriteMany -- the volume can be mounted as read-write by many nodes
ReadWriteOncePod (New only suppots for CSI)-- the volume can be mounted as read-write by a single nodes

Ex:
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv0003
spec:
  capacity:
    storage: 5Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  storageClassName: slow
  mountOptions:
    - hard
    - nfsvers=4.1
  nfs:
    path: /tmp
    server: 172.17.0.2
---------------------------------------------------------------------------------------------------------------
Reclaim Policies:

Retain: When the PersistentVolumeClaim is deleted, the PersistentVolume will still exists, 
and the volume is considered "released". But it is not yet available for another claim because the 
previous claimant's data remains on the volume. An administrator must manually reclaim the volume.
(Data will still remain even we delete the PersistentVolume,PersistentVolumeClaim)

Delete: Delete reclaim policy removes both the PersistentVolume object from Kubernetes, 
as well as the associated storage
(Data will be deleted but the persistancevolume configurations will be in the bounded state)

Recycle: recycle reclaim policy performs a basic scrub (rm -rf /thevolume/*) on the volume 
and makes it available again for a new claim.
(Data will be removed and the persistancevolume can be reused again)
---------------------------------------------------------------------------------------------------------------
Assignments:

1. Read about Pod failures:

a) CrashloopBackoff.
Container inside the pod is getting restarted continuously then it will results as crashloopbackoff.

b) Imagepull failure.
If the image specified in the pod definition is not available, the pod will fail to launch.

c) Network failure.
Network problems, such as disconnections or delays, can cause pods to fail.

d) Resource constrain.
Pods may fail if they do not have sufficient resources like memory or CPU to run.

e) Storage failure:
If a pod requires access to persistent storage and the storage becomes unavailable, the pod may fail.

How to resolve those issues:
https://odsc.medium.com/common-issues-with-kubernetes-deployments-and-how-to-fix-them-dd3b949df87
---------------------------------------------------------------------------------------------------------------

2. Read about emptyDir Volume Mount

An emptyDir volume is first created when a Pod is assigned to a node, and exists as long as that Pod is running on that node.
As the name says, the emptyDir volume is initially empty. All containers in the Pod can read and write the same files 
in the emptyDir volume,though that volume can be mounted at the same or different paths in each container.
When a Pod is removed from a node for any reason, the data in the emptyDir is deleted permanently.

apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  volumes:
  - name: empty
    emptyDir: {}
  containers:
  - name: container1
    image: nginx
    ports:
    - containerPort: 80
    volumeMounts:
    - mountPath: /home
      name: empty
  - name: container2
    image: nginx
    ports:
    - containerPort: 80
    volumeMounts:
    - mountPath: /home
      name: empty

path: /var/lib/kubelet/pods/{podid}/volumes/kubernetes.io~empty-dir/
kubectl exec -it <pod-name> -c <container1_name> -- bash --> To get inside the container 1 created in same pod.
kubectl exec -it <pod-name> -c <container2_name> -- bash --> To get inside the container 2 created in same pod.

---------------------------------------------------------------------------------------------------------------
3. Read about init containers and side car containers

a) init contaier (https://youtu.be/y59KiejIsgs)
A Pod can have multiple containers running apps within it, but it can also have one or more init containers
which are run before the app containers are started.

Init containers are exactly like regular containers, except:
Init containers always run to completion.
Each init container must complete successfully before the next one starts.
If in case init container get filed and the init container will be get restarted.

b) side car containers
Both the main container inside the pod where an application is running and the sidecar container inside the same 
pod both the containers should simultaneously.
They can share the volumes storage along with the main container which allows the main container to to access
the data inside the sidecar container.

UseCase eg:

The application will be running inside the main container and i want to collect the logs from main container
and i want to store it inside the s3 bucket by using sidecar container.
---------------------------------------------------------------------------------------------------------------
4. CrashLoopBackOff (https://devtron.ai/blog/troubleshoot_crashloopbackoff_pod/)
it is one of the common errors in Kubernetes, indicating a pod constantly crashing in an endless loop and either 
unable to get start or fail.
kubectl describe <pod_name>/ kubectl logs <pod_name>

Reasons for CrashLoopBackOff
1. Insufficient resource.
you may be experiencing CrashLoopBackOff errors due to insufficient memory resources.
2. Port already in use:
The port on which we are trying to connect it's already in use with another application/container.
3. Issue with image
another reason could be the docker image you are using may not working properly
4. Probe Failure
The kubelet uses liveness, readiness, and startup probes to keep checks on the container.Then we should check the
probe configuration (i,e increase initialdelaytime, periodseconds, timeoutseconds etc)
5. The application failure.
In this case you will have to look at the application code and debug it
check this:-https: //www.youtube.com/watch?v=D6MZqo55oWU
--------------------------------------------------------------------------------------------------------------
Deployment strategies (https://codefresh.io/learn/software-deployment/top-6-kubernetes-deployment-strategies-and-how-to-choose/)
Github link: https://github.com/DeekshithSN/kubernetes/tree/master/deployment-strategies
youtube link: https://www.youtube.com/watch?v=efiMiaFjtn8

1. Recent Deployment
With this strategy, existing pods from the deployment are terminated and replaced with a new version.
This means the application experiences downtime from the time the old version goes down until the new pods
start successfully and start serving user requests.

2. Rolling Deployment
Rolling deployment is a deployment strategy that updates a running instance of an application to a new version.
All nodes in the target environment are incrementally updated to a new version; the update occurs in pre-specified batches

3. Blue/Green Deployment (Red/Black Deployment)
A blue/green (or red/black) deployment strategy enables you to deploy a new version while avoiding downtime.
Blue represents the current version of the application, while green represents the new version.
This strategy keeps only one version live at any given time. It involves routing traffic to a blue deployment 
while creating and testing a green deployment. After the testing phase is concluded, you start routing traffic 
to the new version. Then, you can keep the blue deployment for a future rollback or decommission it.
command to roll form older version to newer version:
kubectl patch service <labels> -p '{"spec:{"selector":{"version":"v2.0.0"}}}'
kubectl patch service my-app -p '{"spec:{"selector":{"version":"v2.0.0"}}}'

4. Canary Deployment.
Typically, a canary strategy gradually deploys a new application version to the Kubernetes cluster
testing it on a small amount of live traffic.
---------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------
20-12-22 - class 9
---------------------------------------------------------------------------------------------------------------
Environment Variables:

apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 80
    env:
    - name: ENV
      value: dev
    - name: DB
      value: postgress

In this method we are hardcoring the values inside the configuration file if we want to change we need to get
inside the configuration file there we need to change it. user should change based on their use.
We cannont be parameterized them in this methods.
---------------------------------------------------------------------------------------------------------------
ConfigMaps & Secrets:
They are kubernetes objects to store data in key value pairs pods can then use the data as environment variables
or as config files in a volume.
ConfigMaps & Secrets are ways of separating the configuration data from the manifest which makes them more portable. 

(What ever we write inside these configMaps those are human readable any one who has access they can be easily readable.
The same configmap and secrets file can be used for different namespaces by changing the values based on the user
requirement, that make them potable for different environment (i,e devlopment, prod, uat etc) rather hardcoring the values)

apiVersion: v1
kind: ConfigMap
metadata:
  name: username
data:
  user: abc

kubectl create configmap <configmap-name> --from-literal <key>=<value>
kubectl create configmap username --from-literal user=abc
---------------------------------------------------------------------------------------------------------------
Secrets:
Kuberenetes secretes are secured objects to store sensitive data such as passwords, keys etc which are encrypted
in base64
(In this method we can see only the key but value will be not in the human readable.)

apiVersion: v1
kind: Secret
metadata:
  name: password
type: Opaque
data:
  password: test

Either we can use opaque inside the configuration file or we can use echo <value(i,test)> | base64
The result of this we can use in the place of value(i,epassword)=test

echo "abc" | base64 --> To encrypt the value.
echo "abc" | base64 --decode --> To decode the encrypted value from base64
kubectl create secret generic <secret-name> --from-literal <key>=<value>
kubectl create secret generic password --from-literal password=test
---------------------------------------------------------------------------------------------------------------
Mounting ComfigMaps and Secrets as ENV Variables:

apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 80
    env:
    - name: ENV
      value: dev
    - name: DB
      value: postgress
    - name: USER
      valueFrom:
        configMapKeyRef:
          name: username
          key: user
    - name: PASSWORD
      valueFrom:
        secretKeyRef:
          name: password
          key: password
---------------------------------------------------------------------------------------------------------------
Connect the pod with aws RDS instance:
apiVersion: apps/v1
kind: Deployment
metadata:
  name: your-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: your-app
  template:
    metadata:
      labels:
        app: your-app
    spec:
      containers:
      - name: your-app-container
        image: your-app-image:tag
        env:
        - name: DB_HOST
          value: <your-rds-endpoint>
        - name: DB_PORT
          value: "3306" # Change this if your RDS instance uses a different port
        - name: DB_NAME
          value: <your-db-name>
        - name: DB_USERNAME
          valueFrom:
            secretKeyRef:
              name: db-secrets
              key: username
        - name: DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: db-secrets
              key: password
---------------------------------------------------------------------------------------------------------------
How to pull a image from private registory:
step-1:
kubectl create secret docker-registry artifactory-registry-secret \
  --docker-server=<Artifactory_URL> \
  --docker-username=<Artifactory_Username> \
  --docker-password=<Artifactory_Password> \
  --docker-email=<Your_Email>
Replace <Artifactory_URL>, <Artifactory_Username>, <Artifactory_Password>, and <Your_Email> with your 
Artifactory registry information.
step-2:
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - name: my-app-container
        image: <Artifactory_URL>/<repository>/my-image:tag
      imagePullSecrets:
      - name: artifactory-registry-secret

---------------------------------------------------------------------------------------------------------------
Namespace: (kubectl get pod -n kube-system --> pods running inside the kube-system namespace)

Kubernetes namespace allows us to partition our cluster into virtual subdivisions. 
We can create a namespace for different teams, projects etc. 
Objects in different namespaces are invisible to each other which means there won't be any conflicts.
(create logically isolated sections of kubernetes cluster called as namespace
We can restrict the user that for what ammount the user can read a cluser where there is more then one clusters 
for a project)

Kubernetes has four initial namespaces:
• default: This namespace is used for creating objects when a namespace is not specified.
• kube-system: This namespace is used by Kubernetes system to create and run its components.
(All the kubernetse system components that are used to setup the cluster and make sure those are running properly)
• kube-public: The objects inside this namespace are readable by all users including those 
who are not authenticated to the ApiServer. 
• Kube-node-lease: This namespace is a new addition to Kubernetes and it has resources which provide tools to
monitor objects associated with each node.
---------------------------------------------------------------------------------------------------------------
apiVersion: v1
kind: Namespace
metadata:
  name: dev

kubectl create ns <namespace-name> --> To create a namespace
kubectl delete ns <namespace-name> --> To delete a namespace
kubeclt get pods -n <namespace-name> --> To get the pods inside a particular namespace
kubectl get pods --all-namespace --> To get the pods inside all the namespaces
---------------------------------------------------------------------------------------------------------------
Creating objects inside a namespace:

1. Through Manifest Files:

apiVersion: v1
kind: Pod
metadata:
  name: nginx
  namespace: dev
spec:
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 80
---------------------------------------------------------------------------------------------------------------
2. Through kubectl command

kubectl apply -f <.yaml-file> -n dev --> To create a K8S object within a namespace
---------------------------------------------------------------------------------------------------------------
Assignment:
Mount ConfigMap and Secret as Volumes:

How to switch for a perticular namespace and wiseversa.

sudo snap install kubectx --classic.
kubens --> which gives all the namespaces inside the cluster.
kubens <namespace_name> --> To switch between namespaces.
---------------------------------------------------------------------------------------------------------------
How a pod communicate with other pod in another namespace:
# ExternalName Service definition in Namespace A

# Service definition in Namespace A
apiVersion: v1
kind: Service
metadata:
  name: my-service
  namespace: namespace-a
spec:
  selector:
    app: my-app
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
---
apiVersion: v1
kind: Service 
metadata:
  name: my-external-service
  namespace: namespace-a
spec:
  type: ExternalName
  externalName: my-pod.namespace-b.svc.cluster.local
  ports:
    - protocol: TCP
      port: 80
In this example, the my-external-service service in Namespace A acts as an alias to the pod named my-pod in Namespace B. 
Traffic to my-external-service is directed to the pod in Namespace B using DNS resolution.
---------------------------------------------------------------------------------------------------------------
Networking policies:
Network Policies are Kubernetes resources that can be used to block network traffic at the IP address and port level,
it does this by using a set of selectors to filter out and allow traffic from (and to) specific network entities only.

Before applaying network policies need to deploy calico in eks:
link:-https://www.youtube.com/watch?v=u1KUft3fsCk
study this docs:-https://kubernetes.io/docs/concepts/services-networking/network-policies/
21-12-22 - class 10
---------------------------------------------------------------------------------------------------------------
Resource Quotas:

Namespaces don't enforce limitations or quotas automatically for this purpose we can use kubernets quotas to
specify limits for around 15 kubernetes objects.

apiVersion: v1
kind: ResourceQuota
metadata:
  name: rq
spec:
  hard:
    pods: 1
    configmaps: 2
    requests.cpu: 0.5
    limits.cpu: 1
    requests.memory: 256Mi
    limits.memory: 500Mi

kubectl get resourcequots -n <namespace-name>
kubectl edit resourecquota <object-name> -n <namespace-name> --> To check the resourcequotas for namespace.
Limits: The maximum of resources that a namespace can use.
requests: That much ammount of resource will be kept aside for that namespace.
---------------------------------------------------------------------------------------------------------------
Pod with Limits:

apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 80
    resources:
      requests:
        cpu: 0.1
        memory: 64Mi
      limits:
        cpu: 0.25
        memory: 128Mi
---------------------------------------------------------------------------------------------------------------
Securing Kubernetes:

config-file:
Has information about the k8s cluster to which the user try to access and will get authenticated ever time when
user tries to perform any actio in the cluster.
The file which is persent inside the user who ever using the Kubernetes cluster where this file will get 
authenticated every time when ever the user tries to connect with the Kubernetes cluster through api-server.
If the file get lost then the user cannot able to connect for the kubernetes cluster.

Context:
A context is a group of access parameters to enable secure connection to a specific cluster’s API server. 
Each context contains a Kubernetes cluster, a user, and a namespace..
(Details about the kubernetes cluster, which user is trying to connect to the cluster and provides an secure 
connection to the cluster)
---------------------------------------------------------------------------------------------------------------
Types of Users in Kubernetes: 
User Account: We can create users and groups who can connect to the Kubernetes API server. 
kubernetes- admin is the default user
(Users to have access to kubernetes cluster)

Service-Account:
Service accounts are used to give access to processes inside pods to interact with the
Kubernetes API. They can also be used by applications outside the cluster.
(services to have access to kubernetes cluster)

For example, Prometheus monitoring tool which is used to monitor the cluster can be given the access using 
Service-Account.
---------------------------------------------------------------------------------------------------------------
assignment myself: create user account.
---------------------------------------------------------------------------------------------------------------
Creating Service Account:

Step1: Create a Service Account
kubectl create sa test

Step2: Get the secret token name of the service Account
kubectl describe sa test

Step3: Get the token value of the secret
kubectl describe secret <token-name>

Step4: Assign the token value to a Variable
TOKEN="<token-value-from-step3>"

Step5: Assign the credential to the service account
kubectl config set-credentials test --token=$TOKEN

Step6: Creating a Context for the Service Account
kubectl config set-context con --cluster=kubernetes --user=test

commands:
kubectl config --> To check the details about the config file.
kubectl get-contexts --> To check the context that we are using.
---------------------------------------------------------------------------------------------------------------
Commands:

kubectl config get-contexts --> To display available contexts in config
kubectl config use-context con --> To switch to a context
---------------------------------------------------------------------------------------------------------------
RBAC:

RBAC or Role-Based Access Control is an approach in Kubernetes used to add constraints for users, groups, 
and applications to access Kubernetes resources. RBAC basically adds security to the Kubernetes cluster, 
and we can apply it for a specific namespace or to the total cluster.
It was introduced in version 1.8 and uses rbac.authorization.k8s.io API group. 3 important concepts in RBAC.

• Subject: Subject is the entity that needs access. It could be user or group or a service account
(Users who are trying to access the kubernetes cluster i,e service account or user account)
• Resources: Resource is the K8s object that a subject wants to access. It could be pods, deployments, services etc
(This are the permissions for the user to access the api-objects inside a cluster)
• Verbs: Verbs are the actions that a subject can do on resources. It could be the list, delete, create, watch etc
(Actions that subjects can perform on the cluster)
---------------------------------------------------------------------------------------------------------------
Role & Cluster Role:
Role and ClusterRole contains set of rules to access & modify Kubernetes resources. 
Difference between them is Role works in a particular namespace while ClusterRole is cluster wide. 
Basically, we use Role If we want to define permissions inside a namespace and use ClusterRole for cluster wide.

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: cr
rules:
- apiGroups: ["*"]
  resources: ["pods", "secrets"]
  verbs: ["list", "get"]

kubectl create clusterrole cr --resources=pods --resource=secrets --verb=get --verb=list
---------------------------------------------------------------------------------------------------------------
Cluster Role Binding and Role Bindings:

Rolebinding binds the Role to a Subject to access the Resources within a namespace while 
ClusterRoleBinding binds the ClusterRole to a Subject to access the resources cluster-wide.

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: crb
subjects:
- kind: ServiceAccount
  name: test
  namespace: default
roleRef:
  kind: ClusterRole
  name: cr
  apiGroup: rbac.authorization.k8s.io

kubectl create clusterrolebinding crb --serviceaccount=default:test --clusterrole=cr
---------------------------------------------------------------------------------------------------------------
kubectl auth can-i get pods --> To check access
How to check other user/service accounts have access to perform authorization inside the cluster.
https://discuss.kubernetes.io/t/how-to-create-user-in-kubernetes-cluster-and-give-it-access/9101/2 -->
To create and provide access for a user in kubernetes.
---------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------
class- 11
---------------------------------------------------------------------------------------------------------------
EKS Cluster [Windows]: 

Step 1: Install AWS ClI
https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2-windows.html

Step 2: Open cmd.exe as Administrator

Step 3: Instal Chocolatey
https://docs.chocolatey.org/en-us/choco/setup

Step 4: Install eksctl
choco install -y eksctl

Step 5: Install kubectl
choco install kubernetes-cli

Step 6: Create Cluster
eksctl create cluster \
 --name my-cluster \
 --version 1.23 \
 --with-oidc \
 --nodegroup-name worker \
 --region ap-south-1 \
 --node-type t2.medium \
 --managed \
 --ssh-access \
 --ssh-public-key oct-2022 
 
Setting Kube Config File:
aws eks --region ap-south-1 update-kubeconfig --name my-cluster

Cleanup:
eksctl delete cluster --name my-cluster --region ap-south-1
----------------------------------------------------------------------------------------------------------
one you created an eks cluster with an a perticular user inside a concole, if you want to give permissions
for other users then you should add user to the configmaps of auth file inside the kube-system namespaces
----------------------------------------------------------------------------------------------------------

Kubernetes Dashboard [UI]:

Step1: Install Kubernetes Metric Server[eks]
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

Step2: Install Kubernetes Dashboard
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml

Alternatively [https://github.com/kubernetes/dashboard]

Step3: Run command line proxy
kubectl proxy

Step4: Access the Dashboard
http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/

step5: copying the secrete token.
kubectl describe secret -n kubernetes-dashboard
kubectl describe secret kubernetes—dashboard—token—vkk54 -n kuberenetes-dashboard

Note: The UI can only be accessed from the machine where the command is executed

Cleanup:
kubectl delete ns kubernetes-dashboard
----------------------------------------------------------------------------------------------------------
Ingress exposes HTTP and HTTPS routes from outside the cluster to services within the cluster. 
Traffic routing is controlled by rules defined on the Ingress resource.

For the Ingress resource to work, the cluster should have an ingress controller running. 
The famous ingress controllers are Nginx, ALB and GCE

Deploy Ingress Controller [https://kubernetes.github.io/ingress-nginx/deploy/]:

Step1: Install Helm
https://helm.sh/docs/intro/install/

Step2: Deploy Ingress Controller
helm upgrade --install ingress-nginx ingress-nginx \
  --repo https://kubernetes.github.io/ingress-nginx \
  --namespace ingress --create-namespace

Repo Link: https://github.com/artisantek/kubernetes-ingress

apiVersion: apps/v1
kind: Deployment
metadata:
  name: hello
spec:
  selector:
    matchLabels:
      app: hello
  replicas: 2
  template:
    metadata:
      labels:
        app: hello
    spec:
      containers:
      - name: hello
        image: artisantek/k8s-helloworld
---
apiVersion: v1
kind: Service
metadata:
  name: hello
spec:
  type: ClusterIP
  ports:
  - port: 80
    targetPort: 8080
  selector:
    app: hello
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress
spec:
  ingressClassName: nginx
  rules:
  - http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: hello
            port: 
              number: 80

Path based:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: deploy-blue
spec:
  replicas: 1
  selector:
    matchLabels:
      app: blue
  template:
    metadata:
      labels:
        app: blue
    spec:
      volumes:
      - name: webdata
        emptyDir: {}
      initContainers:
      - name: web-content
        image: busybox
        volumeMounts:
        - name: webdata
          mountPath: "/webdata"
        command: ["/bin/sh", "-c", 'echo "<h1><font color=Blue>BLUE APP</font></h1>" > /webdata/index.html']
      containers:
      - image: nginx
        name: nginx
        volumeMounts:
        - name: webdata
          mountPath: "/usr/share/nginx/html"
---
apiVersion: v1
kind: Service
metadata:
  name: blue
spec:
  type: ClusterIP
  selector:
    app: blue
  ports:
  - port: 80
    targetPort: 80

apiVersion: apps/v1
kind: Deployment
metadata:
  name: deploy-green
spec:
  replicas: 1
  selector:
    matchLabels:
      app: green
  template:
    metadata:
      labels:
        app: green
    spec:
      volumes:
      - name: webdata
        emptyDir: {}
      initContainers:
      - name: web-content
        image: busybox
        volumeMounts:
        - name: webdata
          mountPath: "/webdata"
        command: ["/bin/sh", "-c", 'echo "<h1><font color=Green>GREEN APP</font></h1>" > /webdata/index.html']
      containers:
      - image: nginx
        name: nginx
        volumeMounts:
        - name: webdata
          mountPath: "/usr/share/nginx/html"
---
apiVersion: v1
kind: Service
metadata:
  name: green
spec:
  type: ClusterIP
  selector:
    app: green
  ports:
  - port: 80
    targetPort: 80

ingerss rules:

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$2
spec:
  ingressClassName: nginx
  rules:
  - http:
      paths:
      - path: /blue
        pathType: Prefix
        backend:
          service:
            name: blue
            port: 
              number: 80
      - path: /green
        pathType: Prefix
        backend:
          service:
            name: green
            port: 
              number: 80

Cleanup:
kubectl delete ns ingress
----------------------------------------------------------------------------------------------------------
Deploy EBS CSI Driver [Dynamic Volumes]:

Step1: Find the roles attached to the Worker Nodes:
kubectl -n kube-system describe configmap aws-auth

Step2: Attach IAM Policy "AmazonEBSCSIDriverPolicy" to the role

Step3: Deploy CSI Driver
kubectl apply -k "github.com/kubernetes-sigs/aws-ebs-csi-driver/deploy/kubernetes/overlays/stable/?ref=master"

Example:

kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: dynamic
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp2
  fsType: ext4

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc
spec:
  storageClassName: dynamic
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi

apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  volumes:
  - name: persistent-volumes
    persistentVolumeClaim:
      claimName: pvc
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 80
    volumeMounts:
    - mountPath: /home
      name: persistent-volumes
----------------------------------------------------------------------------------------------------------
How to create a new user in aws console and set up in kubernetes cluster by using role, rolebindings & using 
auth file. (https://youtu.be/BuBDYFet0hI)

kubectl edit configmaps aws-auth -n kube-system. --> Location of auth file.

(https://youtu.be/XsFbUgZ0Ad8) --> Creating a useraccount and provide permissions to authorize the cluster.
----------------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------------
How to secure kubernetes cluster:
What is ISTIO:
----------------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------------
Helm Charts:
Helm allowing users to define, install, and upgrade complex applications through a simple set of commands,
it allows us to parameterize the configuration files and makes it reusable.

Components inside a helm:
1) chart.yaml:
Which consist of name of the helm chart and version of helm and app version.
type=application: If we are creating an application we use application
type= liberary: if it is a dependent helm chart we use library 
2) chart directory:
Contain dependencies to other charts that are required by this chart.
3) templates directory:
Which consist of kubernetes objects that are goings to deploy.
4) value.yaml:
Where users can pass parametes for the template.
---------------------------------------------------------------------------------------------------------------
Commands:

helm create <chart-name> --> To create a helm chart template

helm install <chart-name> <folder-name> --> To install a helm chart
helm upgrade <chart-name> <folder-name> --> To upgrade a helm chart
helm list --> To list all the helms.

helm upgrade <chart-name> <folder-name> --set replicaCount=3 --set ports.nodePort=31000 --> To set values from CLI

helm uninstall <chart-name> --> To uninstall a helm chart
---------------------------------------------------------------------------------------------------------------
Example:

deployment.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: webapp
  name: webapp-deployment
spec:
  replicas: {{ .Values.replicaCount }}
  selector:
    matchLabels:
      app: webapp
  template:
    metadata:
      labels:
        app: webapp
    spec:
      containers:
      - image: {{ .Values.image }}
        imagePullPolicy: Always
        name: nodejs
        ports:
        - containerPort: 3080
---------------------------------------------------------------------------------------------------------------
service.yaml

kind: Service
metadata:
  name: webapp-svc
spec:
  type: NodePort
  ports:
  - port: {{ .Values.ports.targetPort }}
    targetPort: 3080
    nodePort: {{ .Values.ports.nodePort }}
  selector:
    app: webapp
---------------------------------------------------------------------------------------------------------------
values.yaml

replicaCount: 3
image: artisantek/useraccount:version1
ports:
  targetPort: 80
  nodePort: 32000
---------------------------------------------------------------------------------------------------------------
Charts.yaml

apiVersion: v2
name: useraccount
description: A Helm chart for Deploying Useraccount Nodejs Application
type: application
version: 0.2.0
appVersion: "version2"
---------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------
Prometheus:
Which is an monitering tool that helps in analyzing the resources usage inside the Kubernetes cluster.
mainly prometheus is used to collect information about the cluster and grafana for visualization through dashboards.
Which is not an user friendly tools so we use grafana for the monitering.
Setup Prometheus:
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update
helm install prometheus prometheus-community/prometheus --set server.persistentVolume.enabled=false
kubectl expose service prometheus-server --type=NodePort --target-port=9090 --name=prometheus-server-node

Setup Grafana:
helm repo add grafana https://grafana.github.io/helm-charts
helm repo update
helm install grafana grafana/grafana --set persistence.enabled=false
kubectl expose service grafana --type=NodePort --target-port=3000 --name=grafana-node
Once the setup is done we need to add the prometheus data base to grafana by selecting data base in garafana
and select the suitable dsahboard.

Grafana Dashboard:
https://grafana.com/grafana/dashboards/3119-kubernetes-cluster-monitoring-via-prometheus/

helm repo list
helm repo remove <repo_name>
---------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------
Corm concept.

---------------------------------------------------------------------------------------------------------------
Backup of etcd: Default path of etcd /var/lib/etcd
https://devopscube.com/backup-etcd-restore-kubernetes/

---------------------------------------------------------------------------------------------------------------
various methods we can update k8s

k8s upgrade:
1.eksctl
2.aws console
3. aws CLI 

These are we need to update:
1. Kubeadm
2. kubelet
3. kubrctl



